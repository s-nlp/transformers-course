# Transformer course
This is the repository of the course on neural networks based on the Transformer architecture targeted at people with experience in Python, Machine Learning, and Deep Learning but little or no experience with Transformers. 

The course features a comprehensive overview of NLP applications but also covers the use of Transformers for other types of data, such as images, networks, or event sequences. The course features $13$ sessions: one lecture per session, one practical session per session, and two homework assignments organized as CodaLab competitions. The first six sessions of the course are devoted to the Transformer and the variations of this architecture (e.g., encoders, decoders, encoder-decoders) as well as different techniques of model tuning. Subsequent sessions are devoted to multilingualism, multimodality, efficiency, tabular data, graphs, and event sequences.

![alt text](https://github.com/s-nlp/transformers-course/blob/main/Course%20structure.png)https://github.com/s-nlp/transformers-course/blob/main/Course%20structure.png)

