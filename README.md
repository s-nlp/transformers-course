# Transformer course
This is the repository of the course on LLMs based on the Transformer architecture targeted at people with experience in Python, Machine Learning, and Deep Learning but little or no experience with Transformers and LLMS. 

The course features a comprehensive overview of NLP applications but also covers the use of Transformers and LLMs for other types of data, such as images, networks, or event sequences. The course features $14$ sessions: one lecture per session, one practical session per session. 

The course covers the concept of attention mechanism, encoder/decoder models, text tokenization and generation, Reinforcement Learning from Human Feedback, Reasoning in LLMs, RAG, LoRA and introduction to AI-agents. 

We expect that after completing the course students:

* Will understand the essence of the main part of Transformers - the attention mechanism, as well as the basic principles of working with texts.

* Familiarize themselves with a diverse range of Natural Language Processing and Machine Learning challenges, equipped with the tools and methodologies necessary to tailor Transformers to address specific problems.

* Develop the ability to correlate a given task encountered during work or study with one of the generalized problems discussed in the course. For instance, mapping the identifying specific words within a document to sequence labeling, and subsequently to multilabel classification or representing word games with chat-bot as a text continuation problem.

* Engage in homework assignments extended beyond the duration typically allotted in similar courses, enabling students to delve deeper into problem-solving, experiment gradually, and compare various approaches to data preprocessing and learning.

## The course Syllabus


* 1. Transformer model
* 2. Transformer-based Encoders
* 3. Classification…with Transformers
* 4. Transformer-based decoders
* 5. Towards ChatGPT
* 6. Efficient Transformers
* 7. RAG with Transformers
* 8. Introduction to AI Agents
* 9. Uncertainty e…n for Transformers
* 10. Uncertainty qu…t generation tasks
* 11. Multilingual language models
* 12. Multimodal dialogue models
* 13. Transformers for tabular data
* 14. Transformers for event sequences

## Editable materials

Raw versions of the editable course materials (e.g., pptx presentations) can be found [here](https://drive.google.com/drive/folders/1rJU9uAyoEcXhutNGeHc8pUIw9gyPzzn7?usp=sharing).

