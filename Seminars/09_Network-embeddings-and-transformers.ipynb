{"cells":[{"cell_type":"markdown","metadata":{"id":"MYpDFySzrk4a"},"source":["# Seminar on Graphs for NLP: Vector representations"]},{"cell_type":"markdown","metadata":{"id":"LlhZKxv4rs2F"},"source":["## Plan for today:\n","\n","#### 0. What a taxonomy is. Taxonomy Enrichment task.\n","#### 1. Graph Neural networks: GCN and GAT\n","#### 2. GATv2\n","#### 3. GraphBERT: Only Attention is Needed for Learning Graph Representations\n","#### 4. GOpenHGNN library"]},{"cell_type":"markdown","metadata":{"id":"6Z0GruYny1kG"},"source":["# 0. Taxonomy\n","\n","A taxonomy is a hierarchical structure of units in terms if class inclusion such that superordinate units in the hierarchy include, or subsume, all items in subordinate units. Taxonomies are typically represented as having tree structures.\n","\n","![](https://www.digital-mr.com/media/cache/51/6f/516f493d37a7b4895f678843b6383e48.png)\n"]},{"cell_type":"markdown","metadata":{"id":"YxGSENmE6IsK"},"source":["Taxonomies can be represented as graphs!\n","\n","Let us download the most popular and well-known taxonomy called WordNet. You may also use the `from nltk.corpus import wordnet as wn`, but keep in mind that you can operate with earlier versions."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Xi-hshM6rL9z"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.0.1+cu118\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n","    status = run_func(*args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n","    return func(self, options, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 417, in run\n","    _, build_failures = build(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/wheel_builder.py\", line 320, in build\n","    wheel_file = _build_one(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/wheel_builder.py\", line 194, in _build_one\n","    wheel_path = _build_one_inside_env(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/wheel_builder.py\", line 241, in _build_one_inside_env\n","    wheel_path = build_wheel_legacy(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/build/wheel_legacy.py\", line 83, in build_wheel_legacy\n","    output = call_subprocess(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/subprocess.py\", line 166, in call_subprocess\n","    line: str = proc.stdout.readline()\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 8, in \u003cmodule\u003e\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n","    return command.main(cmd_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n","    return self._main(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n","    return run(options, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n","    logger.critical(\"Operation cancelled by user\")\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n","    self._log(CRITICAL, msg, args, **kwargs)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1600, in _log\n","    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False,\n","KeyboardInterrupt\n","^C\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C91pcHIwsg_v"},"outputs":[],"source":["!pip install tensorboardX"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"V5VOiYm_2jjR"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n","Requirement already satisfied: numpy\u003e=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n","Requirement already satisfied: scipy\u003e=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n","Requirement already satisfied: smart-open\u003e=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"]}],"source":["!pip install --upgrade gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtnbZKRXvVhk"},"outputs":[],"source":["!gdown --id 1avRebH3BMsolRxmthVFNPoLwyRpAV2tx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORTr2AWjwIvu"},"outputs":[],"source":["!unzip wordnet_n_is_directed_1_en_synsets.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EGS0f0NDdCXv"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'Graph-Bert' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/jwzhanggy/Graph-Bert"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTwSPeZcjmZT"},"outputs":[],"source":["import nltk\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kX-L6-NLeKJ8"},"outputs":[],"source":["from gensim.models.poincare import PoincareModel\n","import numpy as np\n","import time\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ap6gnMUfy146"},"outputs":[],"source":["from nltk.corpus import wordnet as wn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hiwh7Xhcy9hm"},"outputs":[],"source":["import nltk\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2815,"status":"ok","timestamp":1684499816132,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"40OqktlGy5Wd","outputId":"f82d63c2-8ad6-4683-a5bf-4b4978998778"},"outputs":[{"data":{"text/plain":["[Lemma('guy.n.01.guy'),\n"," Lemma('guy.n.01.cat'),\n"," Lemma('guy.n.01.hombre'),\n"," Lemma('guy.n.01.bozo')]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["wn.synset(\"guy.n.01\").lemmas()"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1684499816133,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"XeGdLaEqekbb"},"outputs":[],"source":["path = f\"wordnet_n_is_directed_1_en_synsets/\"\n","\n","link_path = os.path.join(path, \"link\")\n","node_path = os.path.join(path, \"node\")"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":20614,"status":"ok","timestamp":1684499836742,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"C26HaOfGfDkR"},"outputs":[],"source":["id2synset = {}\n","fasttext_dict = {}\n","\n","with open(node_path) as f:\n","    for line in f:\n","        line_split = line.split(\"\\t\")\n","        id2synset[line_split[0].strip()] = line_split[-1].strip()\n","        fasttext_dict[line_split[-1].strip()] = np.array([float(num) for num in line_split[1:-1]])"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1684499836743,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"i7hmkbEXfFIG"},"outputs":[],"source":["link_pairs = set()\n","with open(link_path) as f:\n","    for line in f:\n","        line_split = line.split(\"\\t\")\n","        link_pairs.add((id2synset[line_split[0].strip()], id2synset[line_split[-1].strip()]))"]},{"cell_type":"markdown","metadata":{"id":"0Arh9U60uDH8"},"source":["# 4. Graph Neural Networks"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684499836743,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"46UD5px_rcNb"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":1217,"status":"ok","timestamp":1684499837956,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"B1qcGlrbre_y"},"outputs":[],"source":["import torch_geometric.nn as pyg_nn\n","import torch_geometric.utils as pyg_utils"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":249,"status":"ok","timestamp":1684499838201,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"eM2IXSZMuIq8"},"outputs":[],"source":["import time\n","from datetime import datetime\n","\n","import networkx as nx\n","import numpy as np\n","import torch\n","import torch.optim as optim\n","\n","from torch_geometric.datasets import TUDataset\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.data import DataLoader\n","from torch_geometric.utils import train_test_split_edges\n","import torch_geometric.transforms as T\n","from torch_geometric.data import Data\n","\n","from tensorboardX import SummaryWriter\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"markdown","metadata":{"id":"KPMU0mL6IJEA"},"source":["## Data preparation"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1684499838202,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"2NAxJKWLmkae"},"outputs":[],"source":["from gensim.models.keyedvectors import KeyedVectors\n","\n","fasttext = KeyedVectors(vector_size=300)\n","fasttext.add_vectors(list(fasttext_dict.keys()), list(fasttext_dict.values()))"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1684499838202,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"JL1yQgaD4mio"},"outputs":[],"source":["import networkx as nx"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1684499838203,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"WkLOHE1t4qJo"},"outputs":[],"source":["G = nx.DiGraph()\n","\n","for pair in link_pairs:\n","    G.add_edge(*pair)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1684499838203,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"cgC4chwuIJEC"},"outputs":[],"source":["def create_edge_list(G):\n","    starts = []\n","    ends = []\n","    for left, right in G.edges:\n","        if left in fasttext.key_to_index and right in fasttext.key_to_index:\n","            starts.append(fasttext.key_to_index[left])\n","            ends.append(fasttext.key_to_index[right])\n","    return torch.tensor([starts, ends], dtype=torch.long)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1684499838204,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"ILgb5IF4a2A9"},"outputs":[],"source":["index_to_key = dict(map(reversed, fasttext.key_to_index.items()))"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684499838204,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"f_iX7WH2nCSv"},"outputs":[],"source":["edge_index = create_edge_list(G)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2850,"status":"ok","timestamp":1684499841047,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"6ZJdPogQy1kG","outputId":"7346a26f-df16-43ce-abdd-6baf3ad68823"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u003cipython-input-23-ceafeacad9cc\u003e:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n","  x = torch.tensor([fasttext[index_to_key[int(i)]] for i in index_to_key], dtype=torch.float)\n"]}],"source":["x = torch.tensor([fasttext[index_to_key[int(i)]] for i in index_to_key], dtype=torch.float)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1684499841048,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"WC3VgJ0FyiiH"},"outputs":[],"source":["data = Data(x=x, edge_index=edge_index)\n","#data = train_test_split_edges(data)"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684499841049,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"TCVO4jfK1LFI"},"outputs":[],"source":["from torch_geometric.transforms import RandomLinkSplit"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684499841049,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"lPA8jPvZ1IIn"},"outputs":[],"source":["transform = RandomLinkSplit(is_undirected=True, split_labels=True)\n","train_data, val_data, test_data = transform(data)"]},{"cell_type":"markdown","metadata":{"id":"uCZE_0e0IJED"},"source":["### GCN and GAT Encoder\n","\n","The following code snippet describes the Encoder module with GCN or GAT networks."]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1684499841415,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"I-naFqNRumvk"},"outputs":[],"source":["class Encoder(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels, mode=\"gcn\"):\n","        super(Encoder, self).__init__()\n","        if mode == \"gcn\":\n","            self.conv1 = pyg_nn.GCNConv(in_channels, 2 * out_channels, cached=True)\n","            self.conv2 = pyg_nn.GCNConv(2 * out_channels, out_channels, cached=True)\n","        elif mode == 'gat':\n","            self.conv1 = pyg_nn.GATConv(in_channels, 2 * out_channels)\n","            self.conv2 = pyg_nn.GATConv(2 * out_channels, out_channels)\n","        else:\n","            raise Exception(\"Encoder mode is not recognized, try gcn/gat\")\n","\n","    def forward(self, x, edge_index):\n","        x = F.relu(self.conv1(x, edge_index))\n","        return self.conv2(x, edge_index)\n","\n","def train(epoch):\n","    model.train()\n","    optimizer.zero_grad()\n","    z = model.encode(x, train_pos_edge_index)\n","    loss = model.recon_loss(z, train_pos_edge_index)\n","    loss.backward()\n","    optimizer.step()\n","    writer.add_scalar(\"loss\", loss.item(), epoch)\n","    return loss.item()\n","\n","def test(pos_edge_index, neg_edge_index):\n","    model.eval()\n","    with torch.no_grad():\n","        z = model.encode(x, train_pos_edge_index)\n","    return model.test(z, pos_edge_index, neg_edge_index)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1684499841416,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"eplyGdMwvF57","outputId":"79131632-584e-428b-eea5-b958302b8a2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA availability: True\n"]}],"source":["writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","\n","channels = 64\n","dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('CUDA availability:', torch.cuda.is_available())"]},{"cell_type":"markdown","metadata":{"id":"UHEHYEbcXCFG"},"source":["## Variational Graph Auto-Encoders\n","\n","https://arxiv.org/pdf/1611.07308.pdf\n","\n","The pipeline is working as follows: first, we train a graph autoencoder with GCN or GAT under the hoot. During the evaluation phase, the latent representations of the autoencoder are actually the embeddings we are looking for."]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40195,"status":"ok","timestamp":1684499881605,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"mqZGTsQ7vRQM","outputId":"5b9b671c-3fbc-4389-ef5c-51d9ec00400e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 010, AUC: 0.8353, AP: 0.8220, Loss: 0.9354\n","Epoch: 020, AUC: 0.8730, AP: 0.8669, Loss: 0.8543\n","Epoch: 030, AUC: 0.8932, AP: 0.8880, Loss: 0.8268\n","Epoch: 040, AUC: 0.8973, AP: 0.8962, Loss: 0.8096\n","Epoch: 050, AUC: 0.9029, AP: 0.9037, Loss: 0.7987\n","Epoch: 060, AUC: 0.9050, AP: 0.9073, Loss: 0.7874\n","Epoch: 070, AUC: 0.9077, AP: 0.9111, Loss: 0.7850\n","Epoch: 080, AUC: 0.9117, AP: 0.9145, Loss: 0.7795\n","Epoch: 090, AUC: 0.9107, AP: 0.9150, Loss: 0.7801\n","Epoch: 100, AUC: 0.9115, AP: 0.9160, Loss: 0.7733\n","Epoch: 110, AUC: 0.9115, AP: 0.9169, Loss: 0.7753\n","Epoch: 120, AUC: 0.9122, AP: 0.9175, Loss: 0.7691\n","Epoch: 130, AUC: 0.9133, AP: 0.9187, Loss: 0.7701\n","Epoch: 140, AUC: 0.9138, AP: 0.9198, Loss: 0.7685\n","Epoch: 150, AUC: 0.9141, AP: 0.9200, Loss: 0.7662\n","Epoch: 160, AUC: 0.9143, AP: 0.9207, Loss: 0.7653\n","Epoch: 170, AUC: 0.9146, AP: 0.9212, Loss: 0.7652\n","Epoch: 180, AUC: 0.9140, AP: 0.9206, Loss: 0.7645\n","Epoch: 190, AUC: 0.9153, AP: 0.9222, Loss: 0.7613\n","Epoch: 200, AUC: 0.9150, AP: 0.9219, Loss: 0.7588\n","Epoch: 210, AUC: 0.9151, AP: 0.9225, Loss: 0.7591\n","Epoch: 220, AUC: 0.9157, AP: 0.9229, Loss: 0.7590\n","Epoch: 230, AUC: 0.9162, AP: 0.9234, Loss: 0.7593\n","Epoch: 240, AUC: 0.9160, AP: 0.9234, Loss: 0.7573\n","Epoch: 250, AUC: 0.9157, AP: 0.9232, Loss: 0.7581\n","Epoch: 260, AUC: 0.9156, AP: 0.9232, Loss: 0.7594\n","Epoch: 270, AUC: 0.9151, AP: 0.9227, Loss: 0.7559\n","Epoch: 280, AUC: 0.9149, AP: 0.9230, Loss: 0.7564\n","Epoch: 290, AUC: 0.9151, AP: 0.9233, Loss: 0.7559\n","Epoch: 300, AUC: 0.9148, AP: 0.9231, Loss: 0.7555\n","Epoch: 310, AUC: 0.9149, AP: 0.9227, Loss: 0.7517\n","Epoch: 320, AUC: 0.9138, AP: 0.9227, Loss: 0.7536\n","Epoch: 330, AUC: 0.9138, AP: 0.9227, Loss: 0.7538\n","Epoch: 340, AUC: 0.9137, AP: 0.9224, Loss: 0.7511\n","Epoch: 350, AUC: 0.9126, AP: 0.9214, Loss: 0.7524\n","Epoch: 360, AUC: 0.9124, AP: 0.9214, Loss: 0.7479\n","Epoch: 370, AUC: 0.9122, AP: 0.9212, Loss: 0.7531\n","Epoch: 380, AUC: 0.9128, AP: 0.9220, Loss: 0.7503\n","Epoch: 390, AUC: 0.9129, AP: 0.9224, Loss: 0.7485\n","Epoch: 400, AUC: 0.9126, AP: 0.9220, Loss: 0.7499\n"]}],"source":["model = pyg_nn.GAE(Encoder(300, channels, 'gcn')).to(dev)\n","x, train_pos_edge_index = train_data.x.to(dev), train_data.pos_edge_label_index.to(dev)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","for epoch in range(1, 401):\n","    loss = train(epoch)\n","    auc, ap = test(test_data.pos_edge_label_index, test_data.neg_edge_label_index)\n","    writer.add_scalar(\"AUC\", auc, epoch)\n","    writer.add_scalar(\"AP\", ap, epoch)\n","    if epoch % 10 == 0:\n","        print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}, Loss: {:.4f}'.format(epoch, auc, ap, loss))"]},{"cell_type":"markdown","metadata":{"id":"3__lKOCOXNKj"},"source":["#### Examples\n","\n","Let us see the nearest neighbours for the unseen words from the test set."]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":4488,"status":"ok","timestamp":1684499886087,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"1mWcc2HlvVLf"},"outputs":[],"source":["model.eval()\n","new_x = torch.tensor([fasttext[index_to_key[i]] for i in index_to_key], dtype=torch.float).to(dev)\n","z = model.encode(new_x, train_pos_edge_index)"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":6664,"status":"ok","timestamp":1684499892745,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"HoDN-uTq8bem"},"outputs":[],"source":["id2syns = {}\n","syns2id = {}\n","with open('wordnet_n_is_directed_1_en_synsets/node') as f:\n","    for line in f:\n","        id2syns[line.split()[0]] = line.split()[-1]\n","        syns2id[line.split()[-1]] = line.split()[0]"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1684499892746,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"DGWVX-WL74KV"},"outputs":[],"source":["par2orph = {}\n","orph2par = {}\n","with open('wordnet_n_is_directed_1_en_synsets/link') as f:\n","    for line in f:\n","        par_id = line.split()[0]\n","        child_id = line.split()[-1]\n","        \n","        if \"ORPHAN_\" in id2syns[child_id]:\n","            par2orph[id2syns[par_id]] = id2syns[child_id]\n","            orph2par[id2syns[child_id]] = id2syns[par_id]"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28787,"status":"ok","timestamp":1684499921528,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"c4jW1K0d3Fq1","outputId":"28c0f319-d9c9-4c3a-a726-28d785754225"},"outputs":[{"name":"stdout","output_type":"stream","text":["course.n.04 : [('act.n.05', 0.997), ('act.n.03', 0.997), ('action.n.05', 0.9895), ('pump_action.n.01', 0.9862), ('course.n.04', 0.984), ('movement.n.10', 0.9837), ('ORPHAN_100000000', 0.9815), ('job_action.n.01', 0.9805), ('police_action.n.01', 0.9795), ('scheme.n.01', 0.9763)]\n","==========\n","recovery.n.03 : [('reclamation.n.02', 0.9554), ('redemption.n.01', 0.9545), ('bestowal.n.01', 0.9536), ('salvation.n.04', 0.9499), ('search_and_rescue_mission.n.01', 0.9481), ('contribution.n.03', 0.9468), ('lifesaving.n.01', 0.9464), ('accordance.n.02', 0.9424), ('salvage.n.02', 0.9419), ('salvage.n.03', 0.9419)]\n","==========\n","disappearance.n.01 : [('shading.n.01', 0.9468), ('flit.n.02', 0.9313), ('move.n.05', 0.9283), ('blaze.n.05', 0.9282), ('spot.n.05', 0.9277), ('gradient.n.01', 0.9248), ('difference.n.04', 0.9235), ('rewording.n.01', 0.9231), ('foray.n.02', 0.923), ('turning.n.03', 0.9229)]\n","==========\n","hit.n.03 : [('base_hit.n.01', 0.9958), ('hit.n.05', 0.9927), ('hit.n.01', 0.9927), ('slap.n.01', 0.9862), ('knock.n.03', 0.9846), ('pounding.n.01', 0.9798), ('concussion.n.02', 0.9786), ('zap.n.01', 0.9785), ('sideswipe.n.01', 0.9783), ('ORPHAN_100000003', 0.9783)]\n","==========\n","breach.n.01 : [('act.n.05', 0.9998), ('act.n.03', 0.9998), ('breach.n.01', 0.9991), ('breach.n.02', 0.9979), ('ORPHAN_100000004', 0.9975), ('disappointment.n.02', 0.9963), ('copout.n.01', 0.9954), ('damage.n.01', 0.9949), ('failure.n.05', 0.994), ('special_act.n.01', 0.9928)]\n","==========\n","buying.n.01 : [('buying.n.01', 0.997), ('shopping.n.01', 0.9941), ('ORPHAN_100000005', 0.9937), ('redemption.n.03', 0.9905), ('purchase.n.01', 0.9837), ('obtainment.n.01', 0.9835), ('acquisition.n.01', 0.9818), ('catching.n.03', 0.981), ('capture.n.01', 0.9778), ('reception.n.04', 0.9763)]\n","==========\n","restitution.n.03 : [('financial_gain.n.01', 0.9426), ('fulfillment.n.02', 0.9306), ('relief.n.03', 0.9278), ('punitive_damages.n.01', 0.9272), ('non-cash_expense.n.01', 0.9272), ('entail.n.01', 0.9252), ('entail.n.02', 0.9252), ('realization.n.06', 0.9227), ('actual_damages.n.01', 0.9215), ('nominal_damages.n.01', 0.921)]\n","==========\n","abandonment.n.03 : [('abandonment.n.03', 0.9844), ('sewage_disposal.n.01', 0.976), ('appointment.n.06', 0.9759), ('giving.n.03', 0.9758), ('lending.n.01', 0.9741), ('purification.n.04', 0.9677), ('mine_disposal.n.01', 0.9677), ('accession.n.01', 0.9668), ('ORPHAN_100000007', 0.9665), ('prefixation.n.01', 0.9631)]\n","==========\n","mine_disposal.n.01 : [('rescue_equipment.n.01', 0.8983), ('naval_equipment.n.01', 0.8897), ('materiel.n.01', 0.8883), ('strip_mining.n.01', 0.8858), ('teaching_aid.n.01', 0.8827), ('electronics_intelligence.n.01', 0.8811), ('robotics_equipment.n.01', 0.8809), ('porterage.n.01', 0.8768), ('radiotherapy_equipment.n.01', 0.8764), ('gear.n.04', 0.8753)]\n","==========\n","expiation.n.02 : [('logical_proof.n.01', 0.9956), ('act.n.05', 0.9954), ('act.n.03', 0.9954), ('mathematical_proof.n.01', 0.9954), ('demonstration.n.04', 0.993), ('ORPHAN_100000009', 0.9911), ('punitive_damages.n.01', 0.9907), ('actual_damages.n.01', 0.9901), ('derogation.n.02', 0.9895), ('proof.n.02', 0.9893)]\n","==========\n","rendition.n.04 : [('rewording.n.01', 0.97), ('planning.n.02', 0.97), ('difference.n.04', 0.9655), ('rendering.n.06', 0.9623), ('gradient.n.01', 0.9612), ('broad_interpretation.n.01', 0.9611), ('rendition.n.01', 0.9602), ('ORPHAN_100000010', 0.9601), ('rendering.n.07', 0.9597), ('prior.n.01', 0.9568)]\n","==========\n","depression.n.10 : [('reset_button.n.01', 0.9272), ('nudge.n.01', 0.922), ('depression.n.10', 0.9206), ('push_button.n.01', 0.9003), ('concern.n.02', 0.8992), ('duress.n.01', 0.8927), ('bundling.n.03', 0.8912), ('boost.n.03', 0.8909), ('angst.n.01', 0.8906), ('scruple.n.02', 0.8904)]\n","==========\n","blink.n.01 : [('eye.n.05', 0.9725), ('wet_dream.n.01', 0.9548), ('nightmare.n.02', 0.9546), ('malayan_tapir.n.01', 0.9522), ('tapir.n.01', 0.9367), ('nay.n.01', 0.9352), ('dream.n.01', 0.9346), ('snake.n.01', 0.9232), ('southwestern_toad.n.01', 0.9181), ('decapod.n.02', 0.9175)]\n","==========\n","shooting.n.01 : [('shot.n.12', 0.9981), ('set_shot.n.01', 0.9939), ('pivot_shot.n.01', 0.9935), ('cheap_shot.n.02', 0.9923), ('scoop_shot.n.01', 0.9921), ('dunk.n.01', 0.992), ('foul_shot.n.01', 0.9919), ('jumper.n.08', 0.9909), ('lay-up.n.01', 0.9876), ('shoot.n.02', 0.9873)]\n","==========\n","hit.n.02 : [('base_hit.n.01', 0.9278), ('horn_fly.n.01', 0.9099), ('housefly.n.01', 0.9089), ('tsetse_fly.n.01', 0.9077), ('hit.n.05', 0.9027), ('hit.n.01', 0.9027), ('fly.n.01', 0.8932), ('flip.n.04', 0.8399), ('fling.n.03', 0.8333), ('golf_ball.n.01', 0.8303)]\n","==========\n"]}],"source":["c = 0\n","for word in fasttext.key_to_index:\n","    if \".n.\" not in word:\n","        cur_index = fasttext.key_to_index[word]\n","        tensor_ = torch.tensor([[cur_index]*(len(G.nodes)), [i for i in range(0, len(G.nodes))]])\n","        results = model.decode(z, tensor_)\n","        top10 = list(reversed(sorted([(index_to_key[i], round(float(score.cpu().detach().float()), 4)) for i, score in enumerate(results)], key=lambda x: x[1])))[:10]       \n","        print(orph2par[word], \":\", top10)\n","        print(\"=\"*10)\n","        c += 1\n","        if c == 15:\n","            break"]},{"cell_type":"markdown","metadata":{"id":"ZgbmlZqBIJEN"},"source":["## GraphBERT\n","\n","https://github.com/jwzhanggy/Graph-Bert\n","\n","Yet another model for embedding generation is GraphBert. Instead of feeding large input graph, we train GRAPH-BERT with sampled subgraphs within their local contexts. The input vector embeddings to be fed to the graphtransformer model actually cover four parts: (1) raw feature vector embedding, (2) Weisfeiler-Lehman absolute role embedding, (3) intimacy based relative positional embedding, and (4) hop based relative distance embedding, respectively.\n","\n","GRAPH-BERT is trained with the node attribute reconstruction and structure recovery tasks."]},{"cell_type":"markdown","metadata":{"id":"zo0z3trCa2BC"},"source":["![](https://github.com/jwzhanggy/Graph-Bert/raw/master/result/screenshot/model.png)"]},{"cell_type":"markdown","metadata":{"id":"1fOqFVHLa2BD"},"source":["## Subgraph Sampling"]},{"cell_type":"markdown","metadata":{"id":"5htpR5J7a2BD"},"source":["![](https://i.ibb.co/5cbjJZ6/photo-2021-12-07-16-41-32.jpg)"]},{"cell_type":"markdown","metadata":{"id":"XDsdGrffa2BD"},"source":["## Positional embeddings"]},{"cell_type":"markdown","metadata":{"id":"tOny-pLQa2BE"},"source":["### Weisfeiler-Lehman Absolute Role Embedding\n","\n","![](https://i.ibb.co/bgT7gqb/wl.png)\n","\n","### Intimacy based Relative Positional Embedding\n","\n","![](https://i.ibb.co/34FvCf0/photo-2021-12-07-16-52-30.jpg)\n","\n","### Hop based Relative Distance Embedding\n","![](https://i.ibb.co/tCzRcfK/hops-drawio.png)"]},{"cell_type":"markdown","metadata":{"id":"AomlgEcXa2BE"},"source":["Actually, you are simply expected to run two scripts: `script_1_preprocess.py` and `script_2_pre_train.py`"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":466,"status":"ok","timestamp":1684499921979,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"9DXA9H5la2BF","outputId":"5b6a038f-1f76-44f7-cc54-bfe437cc756d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'Graph-Bert'...\n","remote: Enumerating objects: 450, done.\u001b[K\n","remote: Counting objects: 100% (136/136), done.\u001b[K\n","remote: Compressing objects: 100% (58/58), done.\u001b[K\n","remote: Total 450 (delta 106), reused 79 (delta 78), pack-reused 314\u001b[K\n","Receiving objects: 100% (450/450), 2.23 MiB | 15.41 MiB/s, done.\n","Resolving deltas: 100% (232/232), done.\n"]}],"source":["!git clone https://github.com/jwzhanggy/Graph-Bert.git"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109837,"status":"ok","timestamp":1684500031810,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"ueo45TqXa2BF","outputId":"502f8167-d9fe-474a-aa19-d38d37178cdf"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/Graph-Bert\n","************ Start ************\n","WL, dataset: cora\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 1\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 2\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 3\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 4\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 5\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 6\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 7\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 8\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 9\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 10\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 1\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 2\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 3\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 4\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 5\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 6\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 7\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 8\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 9\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 10\n","Loading cora dataset...\n","************ Finish ************\n"]}],"source":["%cd Graph-Bert\n","!python3 script_1_preprocess.py"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1889,"status":"ok","timestamp":1684500033691,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"DkH0wplPa2BG","outputId":"1b7d14df-31e8-4fbf-e929-2a045e7e06a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/content/Graph-Bert/script_2_pre_train.py\", line 5, in \u003cmodule\u003e\n","    from code.MethodBertComp import GraphBertConfig\n","  File \"/content/Graph-Bert/code/MethodBertComp.py\", line 11, in \u003cmodule\u003e\n","    from transformers.modeling_bert import BertPredictionHeadTransform, BertAttention, BertIntermediate, BertOutput\n","ModuleNotFoundError: No module named 'transformers'\n"]}],"source":["!python3 script_2_pre_train.py"]},{"cell_type":"markdown","metadata":{"id":"HYtahIlwa2BG"},"source":["After the model has been trained, we predict embeddings for the new (unseen words) and their nearest neighbours."]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":551,"status":"error","timestamp":1684500034238,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"vgurRJ-Ca2BH","outputId":"5b448749-33c5-44cf-eb8a-7f413133b700"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-37-85005f0cd4d1\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 9\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/nikishina/Graph-Bert/code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/nikishina/Graph-Bert/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mDatasetLoader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mMethodBertComp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mMethodGraphBertGraphRecovery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMethodGraphBertGraphRecovery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DatasetLoader'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["import os\n","import sys\n","\n","import numpy as np\n","from nltk.corpus import wordnet as wn\n","\n","sys.path.append(\"/home/nikishina/Graph-Bert/code\")\n","sys.path.append(\"/home/nikishina/Graph-Bert/\")\n","from DatasetLoader import DatasetLoader\n","from MethodBertComp import GraphBertConfig\n","from MethodGraphBertGraphRecovery import MethodGraphBertGraphRecovery\n","from MethodGraphBertNodeConstruct import MethodGraphBertNodeConstruct\n","from itertools import combinations\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n","\n","\n","def load_data(dataset_path, k, device):\n","    data_obj = DatasetLoader()\n","    data_obj.dataset_source_folder_path = '/home/nikishina/Graph-Bert/data/' + dataset_path + '/'\n","    data_obj.dataset_name = dataset_path\n","    data_obj.k = k\n","    data_obj.device = device\n","    data_obj.load_all_tag = True\n","    return data_obj.load()\n","\n","\n","def get_query_embedding(word, final_embeddings, index_id_map):\n","    offset, definition = wn.synset(word).offset(), wn.synset(word).definition()\n","    index_of_synset = None\n","\n","    for i, j in index_id_map.items():\n","        if j == offset:\n","            index_of_synset = i\n","            break\n","\n","    query_embedding = final_embeddings[index_of_synset]\n","    return query_embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":30,"status":"aborted","timestamp":1684500034239,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"hbIUXhyia2BH"},"outputs":[],"source":["class GraphBERTEmbeddingsSaver:\n","    def __init__(self, model_name, model, x_size=300, device='cpu', max_index=132, intermediate_size=32,\n","                 num_attention_heads=2, num_hidden_layers=2, y_size=0, residual_type='graph_raw', k=5, nfeature=300):\n","\n","        pretrained_path = './result/PreTrained_GraphBert/' + model_name\n","        bert_config = GraphBertConfig(residual_type=residual_type, k=k, x_size=x_size, y_size=y_size,\n","                                      hidden_size=intermediate_size, intermediate_size=intermediate_size,\n","                                      num_attention_heads=num_attention_heads, num_hidden_layers=num_hidden_layers,\n","                                      max_wl_role_index=max_index, max_hop_dis_index=max_index,\n","                                      max_inti_pos_index=max_index)\n","\n","        self.model = model(bert_config, pretrained_path, device=device)\n","        self.model.eval()\n","        self.nfeature = nfeature\n","\n","    def compute_and_save_embeddings(self, data, test_synsets, index_id_map, id2label, result_dir):\n","        final_embeddings = self.compute_embeddings(data, index_id_map, id2label)\n","        self.save_embeddings(test_synsets, final_embeddings, result_dir)\n","\n","    def compute_embeddings(self, data, index_id_map, id2label):\n","        final_embeddings = np.zeros(shape=(len(index_id_map), self.nfeature), dtype=np.float32)\n","\n","        for _index, raw_f, wl, init, hop in zip(index_id_map, *data):\n","            final_embeddings[_index, :] = np.array(\n","                self.model(raw_f.unsqueeze(0), wl.unsqueeze(0), init.unsqueeze(0), hop.unsqueeze(0))[0]\n","                    .cpu().detach())\n","        return self.get_embeddings_dict(final_embeddings, index_id_map, id2label)\n","\n","    @staticmethod\n","    def get_embeddings_dict(embeddings, index2id_map, id2label):\n","        return {id2label[index]: embeddings[_id] for _id, index in index2id_map.items()}\n","\n","    def save_embeddings(self, test_synsets, embeddings, result_dir):\n","        with open(os.path.join(result_dir, f\"{self.model.__class__.__name__}_model_train_embeddings.txt\"), 'w') as w1:\n","            with open(os.path.join(result_dir, f\"{self.model.__class__.__name__}_model_test_embeddings.txt\"),\n","                      'w') as w2:\n","                for synset_name, embedding in embeddings.items():\n","                    if synset_name in test_synsets:\n","                        text_embedding = \" \".join([str(e) for e in embedding])\n","                        w2.write(f\"{synset_name} {text_embedding}\\n\")\n","                    else:\n","                        text_embedding = \" \".join([str(e) for e in embedding])\n","                        w1.write(f\"{synset_name} {text_embedding}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":29,"status":"aborted","timestamp":1684500034239,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"frvTximja2BI"},"outputs":[],"source":["loaded_data = load_data('wordnet_n_is_directed_1_en_synsets_2.0', 5, 'cpu')\n","dataset = (loaded_data['raw_embeddings'], loaded_data['wl_embedding'], loaded_data['hop_embeddings'],\n","           loaded_data['int_embeddings'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":30,"status":"aborted","timestamp":1684500034240,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"uj1GzbATa2BJ"},"outputs":[],"source":["index_id_map = loaded_data['index_id_map']"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":30,"status":"aborted","timestamp":1684500034240,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"WetYSlefa2BJ"},"outputs":[],"source":["idx_features_labels = np.genfromtxt(\"{}/node\".format('/home/nikishina/Graph-Bert/data/wordnet_n_is_directed_1_en_synsets_2.0/'), dtype=np.dtype(str))\n","id2label = {int(i): j for i, j in zip(idx_features_labels[:, 0], idx_features_labels[:, -1])}"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":30,"status":"aborted","timestamp":1684500034241,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"ahwRGzqZa2BJ"},"outputs":[],"source":["saver = GraphBERTEmbeddingsSaver('wordnet_n_is_directed_1_en_synsets_2.0/node_reconstruct_model', MethodGraphBertNodeConstruct)\n","saver.compute_and_save_embeddings(dataset, new_words, index_id_map, id2label, \"../\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":26,"status":"aborted","timestamp":1684500034241,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"MB-tbeNba2BK"},"outputs":[],"source":["saver = GraphBERTEmbeddingsSaver('wordnet_n_is_directed_1_en_synsets_2.0/node_graph_reconstruct_model', MethodGraphBertGraphRecovery)\n","saver.compute_and_save_embeddings(dataset, new_words, index_id_map, id2label, \"../\")"]},{"cell_type":"markdown","metadata":{"id":"tE-7RzJ1a2BK"},"source":["## View and evaluate results"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":26,"status":"aborted","timestamp":1684500034241,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"zvWT_UAaSW7m"},"outputs":[],"source":["!gdown 1IAfd9tRgtVtdosM5vuDdxh-VSBFp3mzI\n","!gdown 1LItbxEcchOfU4TrlLBZjQweC8jpQ3b3Q\n","!gdown 1VLLLyu9YyLX3uCojiTm_VLtgK2gKCCfW\n","!gdown 1h5sSbFeCJbouH96fKIZDF2xugNiKf3La"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":26,"status":"aborted","timestamp":1684500034241,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"uLCgr2ibUZL9"},"outputs":[],"source":["from gensim.models import KeyedVectors"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":27,"status":"aborted","timestamp":1684500034242,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"bW9Z8hLca2BL"},"outputs":[],"source":["graphBertNode_train = KeyedVectors.load_word2vec_format(\"MethodGraphBertNodeConstruct_model_train_embeddings_.txt\")\n","graphBertNode_test = KeyedVectors.load_word2vec_format(\"MethodGraphBertNodeConstruct_model_test_embeddings.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1684500034242,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"zdnqyA7ia2BL"},"outputs":[],"source":["graphBertNode_train.similar_by_word(\"chocolate_milk.n.01\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1684500034242,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"},"user_tz":-120},"id":"EVHNs_-Za2BM"},"outputs":[],"source":["graphbert_node_predicts = {}\n","\n","for word in fasttext.key_to_index:\n","    if \".n.\" not in word:\n","        graphbert_node_predicts[word] = graphBertNode_train.similar_by_vector(graphBertNode_test[word])"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}