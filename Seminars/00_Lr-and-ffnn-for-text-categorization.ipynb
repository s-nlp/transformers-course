{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKYdJniV50jq"
   },
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJoqhz8EvNAC",
    "outputId": "03e931e1-17a5-4f0f-d48a-8ceb73b6562b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
      "Requirement already up-to-date: torchtext in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YZIzu8To50jt",
    "outputId": "d53a2674-a1e8-4340-f5fa-0914a4771c7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Collecting swifter\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/3b/04bf42b94a22725241b47e0256458cde11f86f97572dd824e011f1ea8b20/swifter-1.0.7.tar.gz (633kB)\n",
      "\u001b[K     |████████████████████████████████| 634kB 6.7MB/s \n",
      "\u001b[?25hCollecting livelossplot\n",
      "  Downloading https://files.pythonhosted.org/packages/57/26/840be243088ce142d61c60273408ec09fa1de4534056a56d6e91b73f0cae/livelossplot-0.5.4-py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from swifter) (1.1.5)\n",
      "Collecting psutil>=5.6.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/da/f7efdcf012b51506938553dbe302aecc22f3f43abd5cffa8320e8e0588d5/psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 22.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: dask[dataframe]>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from swifter) (2.12.0)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.7/dist-packages (from swifter) (4.41.1)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0cloudpickle>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from swifter) (7.6.3)\n",
      "Requirement already satisfied: parso>0.4.0 in /usr/local/lib/python3.7/dist-packages (from swifter) (0.8.1)\n",
      "Requirement already satisfied: bleach>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from swifter) (3.3.0)\n",
      "Collecting modin[ray]>=0.8.1.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/87/118bc738470e51052162f744f3f7e0d9285f6e73be34b49f62c61ae1893d/modin-0.9.1-py3-none-manylinux1_x86_64.whl (579kB)\n",
      "\u001b[K     |████████████████████████████████| 583kB 22.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot) (3.2.2)\n",
      "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot) (2.3.0)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from livelossplot) (5.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->swifter) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->swifter) (2018.9)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->swifter) (1.19.5)\n",
      "Collecting partd>=0.3.10; extra == \"dataframe\"\n",
      "  Downloading https://files.pythonhosted.org/packages/44/e1/68dbe731c9c067655bff1eca5b7d40c20ca4b23fd5ec9f3d17e201a6f36b/partd-1.1.0-py3-none-any.whl\n",
      "Collecting fsspec>=0.6.0; extra == \"dataframe\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/11/f7689b996f85e45f718745c899f6747ee5edb4878cadac0a41ab146828fa/fsspec-0.9.0-py3-none-any.whl (107kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 35.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: toolz>=0.7.3; extra == \"dataframe\" in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]>=2.10.0->swifter) (0.11.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.0.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.10.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.1.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (3.5.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.0.5)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach>=3.1.1->swifter) (0.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach>=3.1.1->swifter) (20.9)\n",
      "Collecting ray<1.2.0,>=1.0.0; extra == \"ray\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/bc/e1ae5b67cd8e0a4ca84f9a59b65b210daf4ed1d9ad69c035f1824aa1256f/ray-1.1.0-cp37-cp37m-manylinux2014_x86_64.whl (48.5MB)\n",
      "\u001b[K     |████████████████████████████████| 48.5MB 63kB/s \n",
      "\u001b[?25hCollecting pyarrow==1.0; extra == \"ray\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/f4/a86a09ae9944ed3c10e2a628dee3e4c37b81f42063ab4f554d6962bc048d/pyarrow-1.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.2MB)\n",
      "\u001b[K     |████████████████████████████████| 17.2MB 211kB/s \n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (2.4.7)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (7.1.2)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.13)\n",
      "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (5.1.1)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.7.4.3)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (4.8.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (2.6.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (54.2.0)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (0.8.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (4.4.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (1.0.18)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (0.7.5)\n",
      "Collecting locket\n",
      "  Downloading https://files.pythonhosted.org/packages/50/b8/e789e45b9b9c2db75e9d9e6ceb022c8d1d7e49b2c085ce8c05600f90a96b/locket-0.2.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fsspec>=0.6.0; extra == \"dataframe\"->dask[dataframe]>=2.10.0->swifter) (3.8.1)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.3.5)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.7.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (2.6.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.2.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.3.1)\n",
      "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.32.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (2.23.0)\n",
      "Collecting aiohttp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 47.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (0.9.0)\n",
      "Collecting aiohttp-cors\n",
      "  Downloading https://files.pythonhosted.org/packages/13/e7/e436a0c0eb5127d8b491a9b83ecd2391c6ff7dcd5548dfaec2080a2340fd/aiohttp_cors-0.7.0-py3-none-any.whl\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.0.2)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (7.1.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (3.12.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (3.0.12)\n",
      "Collecting redis>=3.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 8.7MB/s \n",
      "\u001b[?25hCollecting colorama\n",
      "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
      "Collecting opencensus\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/d6/b952f11b29c3a0cbec5620de3c4260cecd8c4329d83e91587edb48691e15/opencensus-0.7.12-py2.py3-none-any.whl (127kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 54.1MB/s \n",
      "\u001b[?25hCollecting aioredis\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/64/1b1612d0a104f21f80eb4c6e1b6075f2e6aba8e228f46f229cfd3fdac859/aioredis-1.3.1-py3-none-any.whl (65kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 9.5MB/s \n",
      "\u001b[?25hCollecting colorful\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/8e/e386e248266952d24d73ed734c2f5513f34d9557032618c8910e605dfaf6/colorful-0.5.4-py2.py3-none-any.whl (201kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 53.5MB/s \n",
      "\u001b[?25hCollecting py-spy>=0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/a6/52515fe345fad06a567feb0ee3841bface31f00e1e0dcd401aa16b3fc648/py_spy-0.3.5-py2.py3-none-manylinux1_x86_64.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 37.7MB/s \n",
      "\u001b[?25hCollecting gpustat\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/69/d8c849715171aeabd61af7da080fdc60948b5a396d2422f1f4672e43d008/gpustat-0.6.0.tar.gz (78kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 9.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.7->bokeh->livelossplot) (1.1.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->livelossplot) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->livelossplot) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->fsspec>=0.6.0; extra == \"dataframe\"->dask[dataframe]>=2.10.0->swifter) (3.4.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (22.0.3)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.6.1)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.9.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (2020.12.5)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 48.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (20.3.0)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting multidict<7.0,>=4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 51.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.26.2)\n",
      "Collecting opencensus-context==0.1.2\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/33/990f1bd9e7ee770fc8d3c154fc24743a96f16a0e49e14e1b7540cc2fdd93/opencensus_context-0.1.2-py2.py3-none-any.whl\n",
      "Collecting hiredis\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/33/290cea35b09c80b4634773ad5572a8030a87b5d39736719f698f521d2a13/hiredis-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (85kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 10.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (7.352.0)\n",
      "Collecting blessings>=1.6\n",
      "  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.4.3)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.4.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.3)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.7.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.8.4)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.28.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.53.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (4.7.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray<1.2.0,>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (0.4.8)\n",
      "Building wheels for collected packages: swifter, gpustat\n",
      "  Building wheel for swifter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for swifter: filename=swifter-1.0.7-cp37-none-any.whl size=13980 sha256=a5fc3600249571364f0c6266bd7781aee46f987ede76efcd8a54364e482ed240\n",
      "  Stored in directory: /root/.cache/pip/wheels/99/58/39/5b59c5f4d66ce67bf55f0178e0940c964e89e9f60d70376a37\n",
      "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gpustat: filename=gpustat-0.6.0-cp37-none-any.whl size=12621 sha256=062d54b7ac39db7da912c37e646fcadd8ed7a47d2c493d6a47c15d02c57d5989\n",
      "  Stored in directory: /root/.cache/pip/wheels/48/b4/d5/fb5b7f1d040f2ff20687e3bad6867d63155dbde5a7c10f4293\n",
      "Successfully built swifter gpustat\n",
      "\u001b[31mERROR: modin 0.9.1 has requirement pandas==1.2.3, but you'll have pandas 1.1.5 which is incompatible.\u001b[0m\n",
      "Installing collected packages: psutil, multidict, yarl, async-timeout, aiohttp, aiohttp-cors, redis, colorama, opencensus-context, opencensus, hiredis, aioredis, colorful, py-spy, blessings, gpustat, ray, pyarrow, modin, swifter, livelossplot, locket, partd, fsspec\n",
      "  Found existing installation: psutil 5.4.8\n",
      "    Uninstalling psutil-5.4.8:\n",
      "      Successfully uninstalled psutil-5.4.8\n",
      "  Found existing installation: pyarrow 3.0.0\n",
      "    Uninstalling pyarrow-3.0.0:\n",
      "      Successfully uninstalled pyarrow-3.0.0\n",
      "Successfully installed aiohttp-3.7.4.post0 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 blessings-1.7 colorama-0.4.4 colorful-0.5.4 fsspec-0.9.0 gpustat-0.6.0 hiredis-2.0.0 livelossplot-0.5.4 locket-0.2.1 modin-0.9.1 multidict-5.1.0 opencensus-0.7.12 opencensus-context-0.1.2 partd-1.1.0 psutil-5.8.0 py-spy-0.3.5 pyarrow-1.0.0 ray-1.1.0 redis-3.5.3 swifter-1.0.7 yarl-1.6.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "psutil"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install nltk swifter livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IgOD4Ac2p9o3",
    "outputId": "8d6e786d-ab6b-434a-8e41-9b6a2eab1800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting allennlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/43/15d1c4ee4d24fd05ac283e76cc2287a32c02d439e821d8439471f6c869f2/allennlp-2.2.0-py3-none-any.whl (595kB)\n",
      "\u001b[K     |████████████████████████████████| 604kB 4.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch<1.9.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.8.1+cu101)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.10.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
      "Collecting wandb<0.11.0,>=0.10.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/af/4cfe48fe55046181b992251933cff4ceb3bfd71a42838f5fe683683cd925/wandb-0.10.25-py2.py3-none-any.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 10.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
      "Requirement already satisfied: spacy<3.1,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
      "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
      "\u001b[K     |████████████████████████████████| 266kB 26.2MB/s \n",
      "\u001b[?25hCollecting jsonpickle\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/1a/f2db026d4d682303793559f1c2bb425ba3ec0d6fd7ac63397790443f2461/jsonpickle-2.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n",
      "Collecting tensorboardX>=1.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 18.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n",
      "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.0.12)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.7.0)\n",
      "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
      "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
      "Collecting overrides==3.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
      "Requirement already satisfied: torchvision<0.10.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.9.1+cu101)\n",
      "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.41.1)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 21.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
      "Collecting transformers<4.5,>=4.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0MB 51.7MB/s \n",
      "\u001b[?25hCollecting boto3<2.0,>=1.14\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/2d/f094ea90db0ede94ed85cf843da694e18343feed686241af86743f583b00/boto3-1.17.47-py2.py3-none-any.whl (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 48.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.9.0,>=1.6.0->allennlp) (3.7.4.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp) (1.15.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (2.3)\n",
      "Collecting sentry-sdk>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 49.7MB/s \n",
      "\u001b[?25hCollecting subprocess32>=3.5.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 13.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.13)\n",
      "Collecting GitPython>=1.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 47.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (5.8.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.12.4)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
      "Collecting pathtools\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (2.8.1)\n",
      "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (7.1.2)\n",
      "Collecting configparser>=3.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (7.4.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (2.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (54.2.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (3.0.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle->allennlp) (3.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp) (1.0.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.10.0,>=0.8.1->allennlp) (7.1.2)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (20.3.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 45.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.5,>=4.1->allennlp) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.5,>=4.1->allennlp) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 13.9MB/s \n",
      "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 11.0MB/s \n",
      "\u001b[?25hCollecting botocore<1.21.0,>=1.20.47\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/ea/f4a64444f6a4f222ceff3565528c2b9f96083da7bddcc3ef7e96ec5cd77a/botocore-1.20.47-py2.py3-none-any.whl (7.4MB)\n",
      "\u001b[K     |████████████████████████████████| 7.4MB 50.3MB/s \n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.5,>=4.1->allennlp) (2.4.7)\n",
      "Collecting smmap<5,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: jsonnet, overrides, subprocess32, pathtools, sacremoses\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388791 sha256=c21bc3860a63e3383f0fc82d04371fd2192d22b52edd7bdccb3672472721f4d6\n",
      "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=2fd20734edc2db504757a3ddad0ae77e9ce86a941d97ed7d59c9a144679184c9\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=9f8c14425c9806bab071089a1a6d8fa12a0481f7f9b6b0ac6341d70711840b1c\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=b48beb9f6ed2383944dd4514e4327915e7088ad930104bdb97e9305fa93ebb6f\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=12c30fb74ad547d29d8fae813411dbbe242ab6a8de755e88a1794a7c29e6dd71\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
      "Successfully built jsonnet overrides subprocess32 pathtools sacremoses\n",
      "\u001b[31mERROR: botocore 1.20.47 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: sentry-sdk, subprocess32, smmap, gitdb, GitPython, docker-pycreds, pathtools, shortuuid, configparser, wandb, jsonnet, jsonpickle, tensorboardX, overrides, sentencepiece, tokenizers, sacremoses, transformers, jmespath, botocore, s3transfer, boto3, allennlp\n",
      "Successfully installed GitPython-3.1.14 allennlp-2.2.0 boto3-1.17.47 botocore-1.20.47 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-2.0.0 overrides-3.1.0 pathtools-0.1.2 s3transfer-0.3.6 sacremoses-0.0.44 sentencepiece-0.1.95 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 tensorboardX-2.2 tokenizers-0.10.2 transformers-4.4.2 wandb-0.10.25\n"
     ]
    }
   ],
   "source": [
    "!pip install allennlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9176XGP950jw"
   },
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3ydXVQw-50jx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4MfiMeac50j0",
    "outputId": "888c951d-9385-45b2-cb3b-d2be52f90b42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 150\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rezTM_yV50j2"
   },
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "\n",
    "\n",
    "CACHE_DIR = 'cache'\n",
    "if not os.path.isdir(CACHE_DIR):\n",
    "    os.mkdir(CACHE_DIR)\n",
    "    \n",
    "MEMORY = Memory(CACHE_DIR, verbose=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2OtYRCP50j5"
   },
   "source": [
    "# Load and preprocess the corpus annotated with sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2BRCIvtE50j6",
    "outputId": "d2f21464-aa87-4972-b581-77b7617e6465"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yelp_review_polarity_csv.tar.gz: 166MB [00:02, 76.2MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/content/data/yelp_review_polarity_csv/readme.txt',\n",
       " '/content/data/yelp_review_polarity_csv/test.csv',\n",
       " '/content/data/yelp_review_polarity_csv/train.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "\n",
    "datasets = {\n",
    "    'yelp_review_polarity_csv' : 'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbNUpYQ2N3SGlFaDg',\n",
    "    'yelp_review_full_csv' : 'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbZlU4dXhHTFhZQU0',\n",
    "    'amazon_review_polarity_csv' : 'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbaW12WVVZS2drcnM',\n",
    "    'amazon_review_full_csv': 'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbZVhsUnRWRDhETzA'\n",
    "}\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "if not os.path.isdir(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)\n",
    "\n",
    "dataset_name = 'yelp_review_polarity_csv'\n",
    "\n",
    "dataset_tar = download_from_url(datasets[dataset_name], root=DATA_PATH)\n",
    "extracted_files = extract_archive(dataset_tar)\n",
    "extracted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "r22fseYa50j8",
    "outputId": "891e8b38-3c85-45b7-fa4f-86b7abafcd2b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Contrary to other reviews, I have zero complaints about the service or the prices. I have been getting tire service here for the past 5 years now,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Last summer I had an appointment to get new tires and had to wait a super long time. I also went in this week for them to fix a minor problem with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Friendly staff, same starbucks fair you get anywhere else.  Sometimes the lines can get long.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The food is good. Unfortunately the service is very hit or miss. The main issue seems to be with the kitchen, the waiters and waitresses are often...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Even when we didn't have a car Filene's Basement was worth the bus trip to the Waterfront. I always find something (usually I find 3-4 things and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                                                                                                                                   text\n",
       "0      2  Contrary to other reviews, I have zero complaints about the service or the prices. I have been getting tire service here for the past 5 years now,...\n",
       "1      1  Last summer I had an appointment to get new tires and had to wait a super long time. I also went in this week for them to fix a minor problem with...\n",
       "2      2                                                          Friendly staff, same starbucks fair you get anywhere else.  Sometimes the lines can get long.\n",
       "3      1  The food is good. Unfortunately the service is very hit or miss. The main issue seems to be with the kitchen, the waiters and waitresses are often...\n",
       "4      2  Even when we didn't have a car Filene's Basement was worth the bus trip to the Waterfront. I always find something (usually I find 3-4 things and ..."
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(extracted_files[1], header=None, names=['label', 'text']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "P0RmeZ9R50j_"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import swifter\n",
    "from torchtext.vocab import Vocab\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def create_vocab(dataset):  \n",
    "    counter = Counter()\n",
    "    for inst in dataset:\n",
    "        counter.update(inst)\n",
    "\n",
    "    return Vocab(counter, min_freq=1)\n",
    "\n",
    "\n",
    "def vectorize_dataset(dataset, vocab):\n",
    "    result = []\n",
    "    for example in dataset:\n",
    "        result.append([vocab[t] for t in example])\n",
    "    return result\n",
    "\n",
    "\n",
    "@MEMORY.cache\n",
    "def preprocess_dataset(data_path, max_train, max_test, max_dev, random_state=1981):\n",
    "    columns = ['label', 'text']\n",
    "    test_df = pd.read_csv(Path(data_path) / 'test.csv', header=None, names=columns)\n",
    "    train_df = pd.read_csv(Path(data_path) / 'train.csv', header=None, names=columns)\n",
    "\n",
    "    test_df = test_df.sample(n=max_test, random_state=random_state)\n",
    "    train_df = train_df.sample(n=max_dev + max_train, random_state=random_state)\n",
    "    \n",
    "    dev_df = train_df[max_train:]\n",
    "    train_df = train_df[:max_train]\n",
    "    \n",
    "    tokenizer = TweetTokenizer()    \n",
    "    def ling_preprocess(text):\n",
    "        # TODO: add lemmatization\n",
    "        # TODO: add bigrams\n",
    "        return [e.lower() for e in tokenizer.tokenize(text)]\n",
    "    \n",
    "    train_text_preproc = train_df.text.swifter.apply(ling_preprocess)\n",
    "    test_text_preproc = test_df.text.swifter.apply(ling_preprocess)\n",
    "    dev_text_preproc = dev_df.text.swifter.apply(ling_preprocess)\n",
    "    \n",
    "    vocab = create_vocab(train_text_preproc)\n",
    "    \n",
    "    train_token_ids = vectorize_dataset(train_text_preproc, vocab)\n",
    "    test_token_ids = vectorize_dataset(test_text_preproc, vocab)\n",
    "    dev_token_ids = vectorize_dataset(dev_text_preproc, vocab)\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    train_label_ids = label_encoder.fit_transform(train_df.label)\n",
    "    test_label_ids = label_encoder.transform(test_df.label)\n",
    "    dev_label_ids = label_encoder.transform(dev_df.label)\n",
    "    \n",
    "    train_dataset = list(zip(train_label_ids, train_token_ids))\n",
    "    dev_dataset = list(zip(dev_label_ids, dev_token_ids))\n",
    "    test_dataset = list(zip(test_label_ids, test_token_ids))\n",
    "\n",
    "    return (vocab, label_encoder), (train_dataset, test_dataset, dev_dataset), (train_df, test_df, dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohl6mqBr50kB"
   },
   "outputs": [],
   "source": [
    "vocab_label_enc, vectorized_datasets, text_datasets = preprocess_dataset(data_path=Path(DATA_PATH) / dataset_name, \n",
    "                                                                        max_train=4000, \n",
    "                                                                        max_test=4000,\n",
    "                                                                        max_dev=1000)\n",
    "\n",
    "vocab, label_encoder = vocab_label_enc\n",
    "train_dataset, test_dataset, dev_dataset = vectorized_datasets\n",
    "train_text_dataset, test_text_dataset, dev_text_dataset = text_datasets\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "NUM_LABELS = label_encoder.classes_.shape[0]\n",
    "\n",
    "print('Vocab size:', VOCAB_SIZE)\n",
    "print('Number of labels:', NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xM_iZ8rxUCmB"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhGzSFdR50kG"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "def make_bow_vector_count(dataset, vocab_len):\n",
    "    result = torch.zeros((len(dataset), vocab_len), dtype=torch.int32)\n",
    "    for i in range(len(dataset)):\n",
    "        for word in dataset[i]:\n",
    "            result[i, word] += 1\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def train_vectorizer(dataset, vocab_len):\n",
    "    bow = make_bow_vector_count(dataset, vocab_len)\n",
    "    vectorizer = TfidfTransformer()\n",
    "    \n",
    "    vectorizer.fit(bow)\n",
    "    return vectorizer\n",
    "\n",
    "\n",
    "def make_bow_vector_tfidf(dataset, vectorizer):\n",
    "    bow = make_bow_vector_count(dataset, vectorizer.idf_.shape[0])\n",
    "    return torch.Tensor(vectorizer.transform(bow).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3xB2ngu50kJ"
   },
   "outputs": [],
   "source": [
    "VECTORIZER = train_vectorizer(train_dataset, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzEW5sYF50kL",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Logistic regression model in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D55AQPK550kM"
   },
   "source": [
    "## Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DX6ZJSqD50kN"
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionBow(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0nZGSeNxF6O"
   },
   "source": [
    "As we can see below, there is no anything special about nn.Linear and F.linear. They just implement formula: $XW^T+b$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdVqt6BB50kP"
   },
   "outputs": [],
   "source": [
    "nn.Linear??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXXzz7am50kS"
   },
   "outputs": [],
   "source": [
    "F.linear??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVH1Az_K50kU"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0-AzCdj50kW"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionBow(NUM_LABELS, VOCAB_SIZE)\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "    \n",
    "print('Number of parameters', count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZrTSUiV50kZ"
   },
   "outputs": [],
   "source": [
    "# As we can see the model is not trained and outputs garbage.\n",
    "\n",
    "n_top = 3\n",
    "with torch.no_grad():\n",
    "    for sample, text in zip(test_dataset[:n_top], test_text_dataset.iloc[:n_top].text.tolist()):\n",
    "        label, instance = sample\n",
    "        bow_vec = make_bow_vector_tfidf([instance], VECTORIZER)\n",
    "        log_probs = model(bow_vec)\n",
    "        print('Text: ', text[:150], '...', '| Prediction:', log_probs)\n",
    "        print('======================')\n",
    "\n",
    "print('Features')\n",
    "good_word = 'good'\n",
    "print(f'{good_word} : ', next(model.parameters())[:, vocab[good_word]])\n",
    "bad_word = 'nasty'\n",
    "print(f'{bad_word} : ', next(model.parameters())[:, vocab[bad_word]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcjFEY4250kb"
   },
   "source": [
    "## Training model: dummy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73jDpTrN50kc"
   },
   "outputs": [],
   "source": [
    "# Dummy stochastic gradient descent\n",
    "\n",
    "N_EPOCHS = 4\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "loss_function = nn.NLLLoss() # Negative log likelihood loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for sample in tqdm(train_dataset):\n",
    "        label, instance = sample\n",
    "        \n",
    "        # Make our BOW vector and also we must wrap the target in a\n",
    "        # Tensor as an integer. For example, if the target is NEGATIVE, then\n",
    "        # we wrap the integer 0. The loss function then knows that the 0th\n",
    "        # element of the log probabilities is the log probability\n",
    "        # corresponding to NEGATIVE\n",
    "        bow_vec = make_bow_vector_tfidf([instance], VECTORIZER)\n",
    "        target = torch.LongTensor([label])\n",
    "\n",
    "        # Remember that PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Run forward pass.\n",
    "        log_probs = model(bow_vec)\n",
    "\n",
    "        # Compute the loss, gradients, and update the parameters\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJs-GzFj50kf"
   },
   "outputs": [],
   "source": [
    "# We see that our model learned something reasonable\n",
    "n_top = 3\n",
    "with torch.no_grad():\n",
    "    for sample, text in zip(test_dataset[:n_top], test_text_dataset.iloc[:n_top].text.tolist()):\n",
    "        label, instance = sample\n",
    "        bow_vec = make_bow_vector_tfidf([instance], VECTORIZER)\n",
    "        log_probs = model(bow_vec)\n",
    "        print('Text: ', text[:150], '...', '| Prediction:', log_probs)\n",
    "        print('======================')\n",
    "\n",
    "print('Features')\n",
    "good_word = 'good'\n",
    "print(f'{good_word} : ', next(model.parameters())[:, vocab[good_word]])\n",
    "bad_word = 'nasty'\n",
    "print(f'{bad_word} : ', next(model.parameters())[:, vocab[bad_word]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8T0EQrDG50kj"
   },
   "source": [
    "## Train model using DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIBd5zBt50kk"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def generate_batch_bow(batch):\n",
    "    features = make_bow_vector_tfidf([inst for _, inst in batch], VECTORIZER)\n",
    "    labels = torch.LongTensor([label for label, _ in batch])    \n",
    "    return labels, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVyNcecE50kn"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                              collate_fn=generate_batch_bow, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71PrvQR950kp"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 4\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "model = LogisticRegressionBow(NUM_LABELS, VOCAB_SIZE)\n",
    "model.train()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for labels, features in tqdm(train_dataloader):       \n",
    "        model.zero_grad()\n",
    "        log_probs = model(features)\n",
    "        loss = loss_function(log_probs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVPoxJjL50kr"
   },
   "outputs": [],
   "source": [
    "n_top = 3\n",
    "with torch.no_grad():\n",
    "    for sample, text in zip(test_dataset[:n_top], test_text_dataset.iloc[:n_top].text.tolist()):\n",
    "        label, instance = sample\n",
    "        bow_vec = make_bow_vector_tfidf([instance], VECTORIZER)\n",
    "        log_probs = model(bow_vec)\n",
    "        print('Text: ', text[:150], '...', '| Prediction:', log_probs)\n",
    "        print('======================')\n",
    "\n",
    "print('Features')\n",
    "good_word = 'good'\n",
    "print(f'{good_word} : ', next(model.parameters())[:, vocab[good_word]])\n",
    "bad_word = 'nasty'\n",
    "print(f'{bad_word} : ', next(model.parameters())[:, vocab[bad_word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw_Z4Xy750ku"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def test(test_dataset, model, batch_size, device=None):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=generate_batch_bow)\n",
    "    for sample in tqdm(data_loader):\n",
    "        labels, features = sample\n",
    "        if device is not None:\n",
    "            labels = labels.to(device)\n",
    "            features = features.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(features)\n",
    "            loss += loss_function(output, labels).item()\n",
    "            acc += (output.argmax(1) == labels).sum().item()\n",
    "    \n",
    "    return loss / len(test_dataset), acc / len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdno5UMC50kw"
   },
   "outputs": [],
   "source": [
    "loss, acc = test(test_dataset, model, batch_size=200)\n",
    "print(f'Test loss: {loss: .4f}')\n",
    "print(f'Test accuracy: {acc: .4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vfvtb8fr50ky",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6hbCPF-50kz"
   },
   "outputs": [],
   "source": [
    "class FFNNBow(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, hidden_layer_size=5):\n",
    "        super().__init__()\n",
    "        self.hidden_layer = nn.Linear(vocab_size, hidden_layer_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.output_layer = nn.Linear(hidden_layer_size, num_classes)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        intermid = self.hidden_layer(features)\n",
    "        intermid = self.relu(intermid)\n",
    "        intermid = self.output_layer(intermid)\n",
    "        output = F.log_softmax(intermid, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izkemfDY50k1"
   },
   "outputs": [],
   "source": [
    "model = FFNNBow(VOCAB_SIZE, NUM_LABELS).to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "    \n",
    "print('Number of parameters', count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yBbDCXg50k3"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_model_epoch(train_dataset, model, optimizer, bs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(train_dataset, batch_size=bs, \n",
    "                      shuffle=True, collate_fn=generate_batch_bow, \n",
    "                      num_workers=0)\n",
    "    \n",
    "    for sample in tqdm(data):\n",
    "        labels, features = sample\n",
    "        labels = labels.to(device)\n",
    "        features = features.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = loss_function(output, labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss / len(train_dataset)\n",
    "\n",
    "\n",
    "def train(train_dataset, test_dataset, model, optimizer, n_epochs, bs, pred_bs):\n",
    "    print('Training...')\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_model_epoch(train_dataset, model, optimizer, bs=bs)\n",
    "        print(f'Train loss {train_loss:.4f}')\n",
    "    print('Finished training.')\n",
    "\n",
    "    print('Evaluating...')\n",
    "    test_loss, test_acc = test(test_dataset, model, batch_size=pred_bs, device=device)\n",
    "    print(f'Test loss: {test_loss:.4f}')\n",
    "    print(f'Test accuracy: {test_acc: .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baqjAtfV50k5"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 4\n",
    "BATCH_SIZE = 50\n",
    "LEARNING_RATE = 0.01\n",
    "PRED_BATCH_SIZE = 200\n",
    "\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "train(train_dataset, test_dataset, \n",
    "      model, optimizer, \n",
    "      n_epochs=N_EPOCHS, \n",
    "      bs=BATCH_SIZE, \n",
    "      pred_bs=PRED_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XU1puZCL50k-"
   },
   "source": [
    "## Using better gradient descent algorithm (Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PDoIi9N50lA"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 4\n",
    "BATCH_SIZE = 50\n",
    "LEARNING_RATE = 0.01\n",
    "PRED_BATCH_SIZE = 200\n",
    "\n",
    "model = FFNNBow(VOCAB_SIZE, NUM_LABELS).to(device)\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "train(train_dataset, test_dataset, \n",
    "      model, optimizer, \n",
    "      n_epochs=N_EPOCHS, \n",
    "      bs=BATCH_SIZE, \n",
    "      pred_bs=PRED_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mMRJobC50lE"
   },
   "source": [
    "## Adding regularization with L2 weight norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzvCUEsQ50lF"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 4\n",
    "BATCH_SIZE = 50\n",
    "LEARNING_RATE = 0.01\n",
    "PRED_BATCH_SIZE = 200\n",
    "\n",
    "model = FFNNBow(VOCAB_SIZE, NUM_LABELS).to(device)\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "train(train_dataset, test_dataset, \n",
    "      model, optimizer, \n",
    "      n_epochs=N_EPOCHS, \n",
    "      bs=BATCH_SIZE, \n",
    "      pred_bs=PRED_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qP5DYSfD50lL"
   },
   "source": [
    "## Learning rate annealing and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foLzsoDE50lL"
   },
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from livelossplot.outputs import MatplotlibPlot, ExtremaPrinter\n",
    "\n",
    "\n",
    "def train_epoch_adv(train_dataset, \n",
    "                    model, \n",
    "                    optimizer,\n",
    "                    collate_fn, \n",
    "                    loss_fn,\n",
    "                    scheduler=None, \n",
    "                    bs=50):\n",
    "    model.train()\n",
    "    data_loader = DataLoader(train_dataset, batch_size=bs, \n",
    "                             shuffle=True, collate_fn=collate_fn)\n",
    "    train_loss = 0\n",
    "    for cls, tensors in tqdm(data_loader):\n",
    "        if type(tensors) is not tuple:\n",
    "            tensors = (tensors, )\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        tensors, cls = tuple(t.to(device) for t in tensors), cls.to(device)\n",
    "        output = model(*tensors)\n",
    "        loss = loss_fn(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None and not isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step()\n",
    "\n",
    "    return train_loss / len(train_dataset)\n",
    "\n",
    "\n",
    "def test_adv(dataset, model, collate_fn, loss_fn, bs):\n",
    "    model.eval()\n",
    "    data_loader = DataLoader(dataset, batch_size=bs, collate_fn=collate_fn)\n",
    "    test_loss = 0\n",
    "    acc = 0\n",
    "    for cls, tensors in tqdm(data_loader):\n",
    "        if type(tensors) is not tuple:\n",
    "            tensors = (tensors, )\n",
    "            \n",
    "        tensors, cls = tuple(t.to(device) for t in tensors), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(*tensors)\n",
    "            test_loss += loss_fn(output, cls).item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return test_loss / len(dataset), acc / len(dataset)\n",
    "\n",
    "\n",
    "def train_adv(train_dataset, dev_dataset, \n",
    "              model, optimizer, \n",
    "              collate_fn, loss_fn, \n",
    "              bs, pred_bs, \n",
    "              scheduler=None, \n",
    "              draw=False,\n",
    "              tol=-1,\n",
    "              eps=1e-8):\n",
    "    liveloss = PlotLosses(outputs=[MatplotlibPlot(cell_size=(5,3)), ExtremaPrinter()])\n",
    "    \n",
    "    print('Training...')\n",
    "    best_loss = float('inf')\n",
    "    tol_iter = 0\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss = train_epoch_adv(train_dataset, model, optimizer, \n",
    "                                     collate_fn=collate_fn, loss_fn=loss_fn, \n",
    "                                     scheduler=scheduler, bs=bs)\n",
    "        val_loss, val_acc = test_adv(dev_dataset, model, \n",
    "                                     collate_fn=collate_fn, loss_fn=loss_fn, \n",
    "                                     bs=pred_bs)   \n",
    "\n",
    "        if scheduler is not None and isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        if draw:\n",
    "            liveloss.update({'train_loss' : train_loss,\n",
    "                             'val_loss' : val_loss, \n",
    "                             'val_acc' : val_acc})\n",
    "            liveloss.draw()\n",
    "\n",
    "        if tol >= 0:\n",
    "            if best_loss - val_loss > eps:\n",
    "                best_loss = val_loss\n",
    "            else:\n",
    "                tol_iter += 1\n",
    "                if tol_iter > tol:\n",
    "                    break\n",
    "    \n",
    "    print('Finished training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3klLBHn50lO"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 15\n",
    "BATCH_SIZE = 50\n",
    "PRED_SIZE = 100\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "model = FFNNBow(VOCAB_SIZE, NUM_LABELS, 5).to(device)\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "\n",
    "try:\n",
    "    train_adv(train_dataset, dev_dataset, \n",
    "              model, optimizer, \n",
    "              generate_batch_bow, \n",
    "              loss_function, \n",
    "              bs=BATCH_SIZE, \n",
    "              pred_bs=PRED_SIZE, \n",
    "              scheduler=None, \n",
    "              draw=True)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "test_loss, test_acc = test_adv(test_dataset, model, \n",
    "                               generate_batch_bow, \n",
    "                               loss_function, PRED_SIZE)\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOtcpBHp50lQ"
   },
   "outputs": [],
   "source": [
    "# We will use early stopping for preventing the overfitting.\n",
    "# In the real-world scentario, we would like to also track the best model and restore it, when\n",
    "# the validation performance was at maximum.\n",
    "\n",
    "N_EPOCHS = 15\n",
    "BATCH_SIZE = 50\n",
    "PRED_SIZE = 100\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "model = FFNNBow(VOCAB_SIZE, NUM_LABELS).to(device)\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "\n",
    "try:\n",
    "    train_adv(train_dataset, dev_dataset, \n",
    "            model, optimizer, \n",
    "            generate_batch_bow, \n",
    "            loss_function, \n",
    "            bs=BATCH_SIZE, \n",
    "            pred_bs=PRED_SIZE, \n",
    "            scheduler=None, \n",
    "            draw=True, \n",
    "            tol=1)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "\n",
    "test_loss, test_acc = test_adv(test_dataset, model, \n",
    "                               generate_batch_bow, \n",
    "                               loss_function, PRED_SIZE)\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJMS8SE850lR"
   },
   "outputs": [],
   "source": [
    "# Training with smaller learning rate can help to fit model better. But\n",
    "# if we train with small learning rates we also need smaller batch size (so we use batch_size=5). \n",
    "# This all results in a very slow training process.\n",
    "\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 5\n",
    "PRED_SIZE = 100\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "model = FFNNBow(VOCAB_SIZE, NUM_LABELS).to(device)\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "\n",
    "try:\n",
    "    train_adv(train_dataset, \n",
    "            dev_dataset, \n",
    "            model, \n",
    "            optimizer, \n",
    "            generate_batch_bow, \n",
    "            loss_function, \n",
    "            bs=BATCH_SIZE, \n",
    "            pred_bs=PRED_SIZE, \n",
    "            scheduler=None, \n",
    "            draw=True, \n",
    "            tol=1)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "\n",
    "test_loss, test_acc = test_adv(test_dataset, model, \n",
    "                               generate_batch_bow, \n",
    "                               loss_function, PRED_SIZE)\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYkRFaDt50lT"
   },
   "outputs": [],
   "source": [
    "# It is better to use learning rate scheduler. We start from a big learning rate \n",
    "# for faster convergence and closer to the end of the process we reduce LR to a small value\n",
    "\n",
    "N_EPOCHS = 15\n",
    "BATCH_SIZE = 50\n",
    "PRED_SIZE = 100\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "model = FFNNBow(VOCAB_SIZE, NUM_LABELS).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=1)\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "\n",
    "train_adv(train_dataset, dev_dataset, \n",
    "          model, optimizer, \n",
    "          generate_batch_bow, loss_function, \n",
    "          bs=BATCH_SIZE, pred_bs=PRED_SIZE, \n",
    "          scheduler=scheduler, draw=True, tol=5)\n",
    "\n",
    "test_loss, test_acc = test_adv(test_dataset, model, \n",
    "                               generate_batch_bow, \n",
    "                               loss_function, PRED_SIZE)\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2bBw5yd50lU"
   },
   "source": [
    "## Model with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1tv1Ru850lV"
   },
   "outputs": [],
   "source": [
    "class FFNNEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, num_class, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "    \n",
    "    \n",
    "def generate_batch_emb(batch):\n",
    "    texts = [inst for _, inst in batch]\n",
    "    offsets = [0] + [len(text) for text in texts]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    labels = torch.tensor([label for label, _ in batch])\n",
    "    texts =  torch.cat(tuple(torch.tensor(e) for e in texts))\n",
    "    return labels, (texts, offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iu-hLlwj50lX"
   },
   "outputs": [],
   "source": [
    "EMBED_DIM = 20\n",
    "\n",
    "model = FFNNEmbedding(VOCAB_SIZE, NUM_LABELS, EMBED_DIM)\n",
    "print('Number of parameters', count_parameters(model))\n",
    "print(count_parameters(model.fc))\n",
    "print(count_parameters(model.embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVzfR1jZ50lY"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 40\n",
    "EMBED_DIM = 20\n",
    "BATCH_SIZE = 50\n",
    "PRED_SIZE = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "model = FFNNEmbedding(VOCAB_SIZE, NUM_LABELS, EMBED_DIM).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-6)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=1)\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_adv(train_dataset, dev_dataset, \n",
    "          model, optimizer, \n",
    "          generate_batch_emb, loss_function, \n",
    "          bs=BATCH_SIZE, pred_bs=PRED_SIZE, \n",
    "          scheduler=scheduler, draw=True, tol=3)\n",
    "\n",
    "test_loss, test_acc = test_adv(test_dataset, model, \n",
    "                               generate_batch_emb, \n",
    "                               loss_function, PRED_SIZE)\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HM30VEU0lmSm"
   },
   "source": [
    "# Other tips to incorporate in your deep learning pipeline\n",
    "\n",
    "1. Gradient normalization and clipping.\n",
    "2. Stochastic weight averaging / exponential weight averaging.\n",
    "3. Batch normalization in the model and other types of normalizaiton.\n",
    "4. Keep track of the best model on the validation set and restore it when you are doing annealing and at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzQYnMAm1hMO"
   },
   "source": [
    "# Allennlp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ2UU5bW8zmX"
   },
   "source": [
    "## Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auIpQycwm_sp"
   },
   "outputs": [],
   "source": [
    "from allennlp.models import BasicClassifier\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.fields import TextField, LabelField\n",
    "from allennlp.data import Token, Instance\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class ClassificationCsvReader(DatasetReader):\n",
    "    def __init__(self, text_col, label_col, indexer, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "        self.token_indexers = {'tokens' : indexer}\n",
    "        self._label_col = label_col\n",
    "        self._text_col = text_col\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        dataframe = pd.read_csv(file_path, header=None, \n",
    "                                names=[self._label_col, self._text_col])\n",
    "        \n",
    "        for i in dataframe.index:\n",
    "            yield self.text_to_instance(dataframe.loc[i, self._text_col], \n",
    "                                        str(dataframe.loc[i, self._label_col]))\n",
    "\n",
    "    def text_to_instance(self, text, target):\n",
    "        text_field = TextField([Token(e.lower()) for e in self.tokenizer.tokenize(text)],\n",
    "                                   self.token_indexers)\n",
    "        label_field = LabelField(target)\n",
    "        fields = {'tokens': text_field, 'label': label_field}\n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJ702_Xcp01E"
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "\n",
    "\n",
    "indexer = SingleIdTokenIndexer()\n",
    "\n",
    "text_col = 'text'\n",
    "label_col = 'label'\n",
    "reader = ClassificationCsvReader(label_col=label_col, text_col=text_col, indexer=indexer)\n",
    "\n",
    "train_dataset = reader.read(Path(DATA_PATH) / dataset_name / 'train.csv')\n",
    "test_dataset = reader.read(Path(DATA_PATH) / dataset_name / 'test.csv')\n",
    "\n",
    "train_dataset = list(tqdm(islice(train_dataset, 4000)))\n",
    "test_dataset = list(tqdm(islice(test_dataset, 4000)))\n",
    "\n",
    "print(next(iter(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hai0GqpDusZO"
   },
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_dataset, min_count={\"tokens\": 1})\n",
    "print('Vocabulary size:', vocab.get_vocab_size('tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYZzWTEwvpbF"
   },
   "outputs": [],
   "source": [
    "from allennlp.training import GradientDescentTrainer\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n",
    "\n",
    "EMBEDDING_DIM = 20\n",
    "\n",
    "embedder = Embedding(embedding_dim=EMBEDDING_DIM, vocab=vocab)\n",
    "text_field_embedder = BasicTextFieldEmbedder({'tokens': embedder})\n",
    "encoder = BagOfEmbeddingsEncoder(embedding_dim=EMBEDDING_DIM, averaged=True)\n",
    "model = BasicClassifier(vocab, \n",
    "                        text_field_embedder=text_field_embedder, \n",
    "                        seq2vec_encoder=encoder, \n",
    "                        dropout=0.1, \n",
    "                        num_labels=vocab.get_vocab_size('labels'))\n",
    "model = model.to(device)\n",
    "print('Number of paramters:', count_parameters(model))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWhog_zq17Mo"
   },
   "outputs": [],
   "source": [
    "from allennlp.data.data_loaders import SimpleDataLoader\n",
    "from allennlp.training.learning_rate_schedulers import ReduceOnPlateauLearningRateScheduler\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.01,\n",
    "    weight_decay=1e-6,\n",
    ")\n",
    "\n",
    "train_data_loader = SimpleDataLoader(\n",
    "    instances=train_dataset, batch_size=50\n",
    ")\n",
    "train_data_loader.index_with(vocab)\n",
    "\n",
    "test_data_loader = SimpleDataLoader(\n",
    "    instances=test_dataset, batch_size=200\n",
    ")\n",
    "test_data_loader.index_with(vocab)\n",
    "\n",
    "\n",
    "lr_scheduler = ReduceOnPlateauLearningRateScheduler(optimizer, factor=0.5, patience=1)\n",
    "\n",
    "trainer = GradientDescentTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_data_loader,\n",
    "    validation_data_loader=test_data_loader,\n",
    "    num_epochs=40,\n",
    "    cuda_device=device,\n",
    "    learning_rate_scheduler=lr_scheduler,\n",
    "    num_gradient_accumulation_steps=1,\n",
    "    serialization_dir=None,\n",
    "    patience=3\n",
    ")\n",
    "\n",
    "try:\n",
    "    metrics = trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iACTgeZ-4nem"
   },
   "outputs": [],
   "source": [
    "from allennlp.training.util import evaluate\n",
    "\n",
    "evaluate(model, test_data_loader, cuda_device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaQllF5250lc"
   },
   "source": [
    "# Quick tip: In case of insufficient CUDA memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MH0Rjj5K50ld"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of week2_seminar_preview.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
